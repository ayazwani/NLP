{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize , word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My nane is, Mr. Ayaz Wani. I live at noushera , srinagar , kashmir!. How are all of you guys?all well?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My nane is, Mr. Ayaz Wani.', 'I live at noushera , srinagar , kashmir!.', 'How are all of you guys?all well?']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My nane is, Mr. Ayaz Wani.\n",
      "I live at noushera , srinagar , kashmir!.\n",
      "How are all of you guys?all well?\n"
     ]
    }
   ],
   "source": [
    "for i in (sent_tokenize(text)):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'nane', 'is', ',', 'Mr.', 'Ayaz', 'Wani', '.', 'I', 'live', 'at', 'noushera', ',', 'srinagar', ',', 'kashmir', '!', '.', 'How', 'are', 'all', 'of', 'you', 'guys', '?', 'all', 'well', '?']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = \"this is an example to show stop words filtering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'than', 'so', \"needn't\", 'our', \"weren't\", 'if', 'should', 'll', 'after', 'about', 'has', \"doesn't\", 'doing', 'only', 'whom', 'themselves', 'your', 'ain', 'own', 'then', 'you', 'how', 'm', 'of', 'up', 'other', \"haven't\", 'on', 'she', 'they', 'were', 'needn', \"it's\", \"shan't\", 'there', 'from', 'both', 'hasn', 'ours', 'himself', \"you'd\", 'did', 'd', 'further', 'its', 'over', 'shan', 'hadn', 'being', 'more', \"isn't\", 'some', 'below', 'that', 'haven', 'he', 'hers', 'which', 'do', 've', 'don', 'itself', 'weren', \"that'll\", 'aren', 'same', 'had', 'those', 'at', 'wouldn', 'ourselves', 'yourself', 'when', \"don't\", 'wasn', 'is', 'to', 'who', 'most', \"should've\", 'too', 'isn', 'him', 'me', 'nor', 'be', 'for', 'a', \"she's\", 'off', 'all', 'why', 'such', 't', 're', 'we', 'y', \"couldn't\", 'not', 'against', \"hadn't\", 'my', 'through', 'have', 'once', 'myself', 'before', 'out', \"wouldn't\", 'yours', 'didn', 'the', 'between', \"hasn't\", 'with', 'where', 'does', 'by', \"you'll\", \"you're\", 'theirs', \"aren't\", 'shouldn', 'them', 'it', 'just', 'because', 'few', 'no', 'can', 'her', 'was', 'been', 'until', \"wasn't\", 'will', 'this', 'are', 'in', 'having', 'ma', 'very', 'while', 'couldn', \"you've\", 'as', 'am', 'now', 'but', \"didn't\", \"won't\", 'under', 'above', 'during', 'his', 'these', 'their', 'down', 'mightn', 'into', 'or', 'here', 'what', 'yourselves', 'doesn', \"mightn't\", 'each', \"mustn't\", 'any', 'and', 's', 'herself', 'o', 'won', 'again', 'mustn', \"shouldn't\", 'an', 'i'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(ex)\n",
    "\n",
    "fs = []\n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        fs.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'show', 'stop', 'words', 'filtering']\n"
     ]
    }
   ],
   "source": [
    "print(fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = [word for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example', 'show', 'stop', 'words', 'filtering']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stemming \n",
    "i was taking a ride in the car \n",
    "\n",
    "i was riding a car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "exwords = [\"python\" ,\"pythoner\" , \"pythoning\",\"pythoned\",\"pythonly\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for word in exwords:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "everyon\n",
      "happi\n",
      "hap\n",
      "while\n",
      "you\n",
      "are\n",
      "happili\n",
      "and\n",
      "happyer\n",
      "do\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# sentence stemmoig\n",
    "exsent = \"hi everyone happy happing while you are happily and happyer doing gooded\"\n",
    "\n",
    "words = word_tokenize(exsent)\n",
    "\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "Reg_stemmer = RegexpStemmer('ing')\n",
    "Reg_stemmer.stem('ingeating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "Lanc_stemmer = LancasterStemmer()\n",
    "Lanc_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bonjour'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "French_stemmer = SnowballStemmer('french')\n",
    "French_stemmer.stem ('Bonjoura')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('believes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer #unsupervised machine learning tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttext = state_union.raw(\"2005-GWBush.txt\")\n",
    "stext = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_sent_tokenizer= PunktSentenceTokenizer(ttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(stext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in tokenized[:5]:\n",
    "#     words = nltk.word_tokenize(w)\n",
    "#     tagged = nltk.pos_tag(words)\n",
    "#     #print(tagged)\n",
    "#     chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
    "#     chunkParser = nltk.RegexpParser(chunkGram)\n",
    "#     chunked = chunkParser.parse(tagged)\n",
    "# #chunked.draw()   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('cats',pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['won', '’', 't']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('won’t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['online.com',\n",
       " 'provides',\n",
       " 'high',\n",
       " 'quality',\n",
       " 'technical',\n",
       " 'tutorials',\n",
       " 'for',\n",
       " 'free',\n",
       " '.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer_wrd = TreebankWordTokenizer()\n",
    "tokenizer_wrd.tokenize('online.com provides high quality technical tutorials for free.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'allow',\n",
       " 'you',\n",
       " 'to',\n",
       " 'go',\n",
       " 'home',\n",
       " 'early',\n",
       " 'how',\n",
       " \"'\",\n",
       " 're',\n",
       " 'you',\n",
       " 'all',\n",
       " 'good',\n",
       " 'ma',\n",
       " \"'\",\n",
       " 'am']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(\" I can't allow you to go home early how're you all good ma'am\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression to tokenize on whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"won't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "tokenizer.tokenize(\"won't is a contraction.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"can't\", 'is', 'a', 'contraction']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"can't is a contraction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"won't is a contraction.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer('/s+' , gaps = True)\n",
    "tokenizer.tokenize(\"won't is a contraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.corpus import webtext\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = webtext.raw('token1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Guy: How old are you?\\nHipster girl: You know, I never answer that question. Because to me, it's about\\nhow mature you are, you know? I mean, a fourteen year old could be more mature\\nthan a twenty-five year old, right? I'm sorry, I just never answer that question.\\nGuy: But, uh, you're older than eighteen, right?\\nHipster girl: Oh, yeah.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G\n",
      "H\n",
      "B\n",
      "I\n",
      "I\n",
      "G\n",
      "H\n"
     ]
    }
   ],
   "source": [
    "sents_2 = sent_tokenize(text)\n",
    "#print(sentence)\n",
    "for sentence in sents_2:\n",
    "    print(sentence[0])    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G\n",
      "H\n",
      "B\n",
      "I\n",
      "I\n",
      "G\n",
      "H\n"
     ]
    }
   ],
   "source": [
    "sent_tokenizer = PunktSentenceTokenizer(text)\n",
    "sents_1 = sent_tokenizer.tokenize(text)\n",
    "for sent in sents_1:\n",
    "    print(sent[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "syn = wn.synsets('dog')[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('andiron.n.01')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metal supports for logs in a fireplace'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "syn = wn.synsets('dog')[0]\n",
    "syn.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('canine.n.02'), Synset('domestic_animal.n.01')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms()#.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('entity.n.01')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.root_hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('dog.n.01.dog'),\n",
       " Lemma('dog.n.01.domestic_dog'),\n",
       " Lemma('dog.n.01.Canis_familiaris')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = wn.synsets('dog')[0]\n",
    "lemmas = syn.lemmas()\n",
    "(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn1 = wn.synset('good.n.02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('good.n.02')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evil'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonym1 = syn1.lemmas()[0].antonyms()[0]\n",
    "antonym1.name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quality of being morally wrong in principle or practice'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "antonym1.synset().definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn2 = wn.synset('good.a.01')\n",
    "antonym2 = syn2.lemmas()[0].antonyms()[0]\n",
    "antonym2.name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy\n",
    "\n",
    "https://realpython.com/natural-language-processing-spacy-python/#word-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
     ]
    }
   ],
   "source": [
    "introduction_text = ('all is about Natural'\n",
    "     ' Language Processing in Spacy.')\n",
    "introduction_doc = nlp(introduction_text)\n",
    " # Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thsis', 'is', 'an', 'introduction', 'to', 'spaCy', 'by', 'ayaz', 'wnai', '.', 'All', 'is', 'well', 'that', 'ends', 'well', '.', 'NLP', 'is', 'good.never', 'say', 'never', '.']\n"
     ]
    }
   ],
   "source": [
    "file_name = 'introduction.txt'\n",
    "introduction_file_text = open(file_name).read()\n",
    "introduction_file_doc = nlp(introduction_file_text)\n",
    "# Extract tokens for the given doc\n",
    "print ([token.text for token in introduction_file_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about_text = (\"\"\"\"The Internet protocol suite is the conceptual model and set of communications protocols used in the Internet and similar computer networks. \n",
    "              It is commonly known as TCP/IP because the foundational protocols in the suite are \n",
    "              the Transmission Control Protocol (TCP) and the Internet Protocol (IP). During its development, \n",
    "              versions of it were known as the Department of Defense (DoD) model because the development \n",
    "              of the networking method was funded bythe United States Department of Defense through DARPA. Its implementation is a protocol stack.\"\"\")\n",
    "about_doc = nlp(about_text)\n",
    "sentences = list(about_doc.sents)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The Internet protocol suite is the conceptual model and set of communications protocols used in the Internet and similar computer networks. \n",
      "              \n",
      "It is commonly known as TCP/IP because the foundational protocols in the suite are \n",
      "              the Transmission Control Protocol (TCP) and the Internet Protocol (IP).\n",
      "During its development, \n",
      "              versions of it were known as the Department of Defense (DoD) model because the development \n",
      "              of the networking method was funded bythe United States Department of Defense through DARPA.\n",
      "Its implementation is a protocol stack.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "    print (sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_custom_boundaries(doc):\n",
    "    # Adds support to use `...` as the delimiter for sentence detection\n",
    "     for token in doc[:-1]:\n",
    "        if token.text == '...':\n",
    "            doc[token.i+1].is_sent_start = True\n",
    "        return doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ellipsis_text = ('Gus, can you, ... never mind, I forgot'\n",
    "                  ' what I was saying. So, do you think'\n",
    "                  ' we should ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ... never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "custom_nlp = spacy.load('en_core_web_sm')\n",
    "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
    "for sentence in custom_ellipsis_sentences:\n",
    "     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus, can you, ... never mind, I forgot what I was saying.\n",
      "So, do you think we should ...\n"
     ]
    }
   ],
   "source": [
    "# Sentence Detection with no customization\n",
    "ellipsis_doc = nlp(ellipsis_text)\n",
    "ellipsis_sentences = list(ellipsis_doc.sents)\n",
    "for sentence in ellipsis_sentences:\n",
    "     print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization in spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 0\n",
      "The 1\n",
      "Internet 5\n",
      "protocol 14\n",
      "suite 23\n",
      "is 29\n",
      "the 32\n",
      "conceptual 36\n",
      "model 47\n",
      "and 53\n",
      "set 57\n",
      "of 61\n",
      "communications 64\n",
      "protocols 79\n",
      "used 89\n",
      "in 94\n",
      "the 97\n",
      "Internet 101\n",
      "and 110\n",
      "similar 114\n",
      "computer 122\n",
      "networks 131\n",
      ". 139\n",
      "\n",
      "               141\n",
      "It 156\n",
      "is 159\n",
      "commonly 162\n",
      "known 171\n",
      "as 177\n",
      "TCP 180\n",
      "/ 183\n",
      "IP 184\n",
      "because 187\n",
      "the 195\n",
      "foundational 199\n",
      "protocols 212\n",
      "in 222\n",
      "the 225\n",
      "suite 229\n",
      "are 235\n",
      "\n",
      "               239\n",
      "the 254\n",
      "Transmission 258\n",
      "Control 271\n",
      "Protocol 279\n",
      "( 288\n",
      "TCP 289\n",
      ") 292\n",
      "and 294\n",
      "the 298\n",
      "Internet 302\n",
      "Protocol 311\n",
      "( 320\n",
      "IP 321\n",
      ") 323\n",
      ". 324\n",
      "During 326\n",
      "its 333\n",
      "development 337\n",
      ", 348\n",
      "\n",
      "               350\n",
      "versions 365\n",
      "of 374\n",
      "it 377\n",
      "were 380\n",
      "known 385\n",
      "as 391\n",
      "the 394\n",
      "Department 398\n",
      "of 409\n",
      "Defense 412\n",
      "( 420\n",
      "DoD 421\n",
      ") 424\n",
      "model 426\n",
      "because 432\n",
      "the 440\n",
      "development 444\n",
      "\n",
      "               456\n",
      "of 471\n",
      "the 474\n",
      "networking 478\n",
      "method 489\n",
      "was 496\n",
      "funded 500\n",
      "bythe 507\n",
      "United 513\n",
      "States 520\n",
      "Department 527\n",
      "of 538\n",
      "Defense 541\n",
      "through 549\n",
      "DARPA 557\n",
      ". 562\n",
      "Its 564\n",
      "implementation 568\n",
      "is 583\n",
      "a 586\n",
      "protocol 588\n",
      "stack 597\n",
      ". 602\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "    print (token, token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" 0 \" False True False \" False\n",
      "The 1 The  True False False Xxx True\n",
      "Internet 5 Internet  True False False Xxxxx False\n",
      "protocol 14 protocol  True False False xxxx False\n",
      "suite 23 suite  True False False xxxx False\n",
      "is 29 is  True False False xx True\n",
      "the 32 the  True False False xxx True\n",
      "conceptual 36 conceptual  True False False xxxx False\n",
      "model 47 model  True False False xxxx False\n",
      "and 53 and  True False False xxx True\n",
      "set 57 set  True False False xxx False\n",
      "of 61 of  True False False xx True\n",
      "communications 64 communications  True False False xxxx False\n",
      "protocols 79 protocols  True False False xxxx False\n",
      "used 89 used  True False False xxxx True\n",
      "in 94 in  True False False xx True\n",
      "the 97 the  True False False xxx True\n",
      "Internet 101 Internet  True False False Xxxxx False\n",
      "and 110 and  True False False xxx True\n",
      "similar 114 similar  True False False xxxx False\n",
      "computer 122 computer  True False False xxxx False\n",
      "networks 131 networks True False False xxxx False\n",
      ". 139 .  False True False . False\n",
      "\n",
      "               141 \n",
      "               False False True \n",
      "     False\n",
      "It 156 It  True False False Xx True\n",
      "is 159 is  True False False xx True\n",
      "commonly 162 commonly  True False False xxxx False\n",
      "known 171 known  True False False xxxx False\n",
      "as 177 as  True False False xx True\n",
      "TCP 180 TCP True False False XXX False\n",
      "/ 183 / False True False / False\n",
      "IP 184 IP  True False False XX False\n",
      "because 187 because  True False False xxxx True\n",
      "the 195 the  True False False xxx True\n",
      "foundational 199 foundational  True False False xxxx False\n",
      "protocols 212 protocols  True False False xxxx False\n",
      "in 222 in  True False False xx True\n",
      "the 225 the  True False False xxx True\n",
      "suite 229 suite  True False False xxxx False\n",
      "are 235 are  True False False xxx True\n",
      "\n",
      "               239 \n",
      "               False False True \n",
      "     False\n",
      "the 254 the  True False False xxx True\n",
      "Transmission 258 Transmission  True False False Xxxxx False\n",
      "Control 271 Control  True False False Xxxxx False\n",
      "Protocol 279 Protocol  True False False Xxxxx False\n",
      "( 288 ( False True False ( False\n",
      "TCP 289 TCP True False False XXX False\n",
      ") 292 )  False True False ) False\n",
      "and 294 and  True False False xxx True\n",
      "the 298 the  True False False xxx True\n",
      "Internet 302 Internet  True False False Xxxxx False\n",
      "Protocol 311 Protocol  True False False Xxxxx False\n",
      "( 320 ( False True False ( False\n",
      "IP 321 IP True False False XX False\n",
      ") 323 ) False True False ) False\n",
      ". 324 .  False True False . False\n",
      "During 326 During  True False False Xxxxx True\n",
      "its 333 its  True False False xxx True\n",
      "development 337 development True False False xxxx False\n",
      ", 348 ,  False True False , False\n",
      "\n",
      "               350 \n",
      "               False False True \n",
      "     False\n",
      "versions 365 versions  True False False xxxx False\n",
      "of 374 of  True False False xx True\n",
      "it 377 it  True False False xx True\n",
      "were 380 were  True False False xxxx True\n",
      "known 385 known  True False False xxxx False\n",
      "as 391 as  True False False xx True\n",
      "the 394 the  True False False xxx True\n",
      "Department 398 Department  True False False Xxxxx False\n",
      "of 409 of  True False False xx True\n",
      "Defense 412 Defense  True False False Xxxxx False\n",
      "( 420 ( False True False ( False\n",
      "DoD 421 DoD True False False XxX False\n",
      ") 424 )  False True False ) False\n",
      "model 426 model  True False False xxxx False\n",
      "because 432 because  True False False xxxx True\n",
      "the 440 the  True False False xxx True\n",
      "development 444 development  True False False xxxx False\n",
      "\n",
      "               456 \n",
      "               False False True \n",
      "     False\n",
      "of 471 of  True False False xx True\n",
      "the 474 the  True False False xxx True\n",
      "networking 478 networking  True False False xxxx False\n",
      "method 489 method  True False False xxxx False\n",
      "was 496 was  True False False xxx True\n",
      "funded 500 funded  True False False xxxx False\n",
      "bythe 507 bythe  True False False xxxx False\n",
      "United 513 United  True False False Xxxxx False\n",
      "States 520 States  True False False Xxxxx False\n",
      "Department 527 Department  True False False Xxxxx False\n",
      "of 538 of  True False False xx True\n",
      "Defense 541 Defense  True False False Xxxxx False\n",
      "through 549 through  True False False xxxx True\n",
      "DARPA 557 DARPA True False False XXXX False\n",
      ". 562 .  False True False . False\n",
      "Its 564 Its  True False False Xxx True\n",
      "implementation 568 implementation  True False False xxxx False\n",
      "is 583 is  True False False xx True\n",
      "a 586 a  True False False x True\n",
      "protocol 588 protocol  True False False xxxx False\n",
      "stack 597 stack True False False xxxx False\n",
      ". 602 . False True False . False\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "     print (token, token.idx, token.text_with_ws,\n",
    "            token.is_alpha, token.is_punct, token.is_space,\n",
    "            token.shape_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_with_ws prints token text with trailing space (if present).\n",
    "\n",
    "is_alpha detects if the token consists of alphabetic characters or not.\n",
    "\n",
    "is_punct detects if the token is a punctuation symbol or not.\n",
    "\n",
    "is_space detects if the token is a space or not.\n",
    "\n",
    "shape_ prints out the shape of the word.\n",
    "\n",
    "is_stop detects if the token is a stop word or not.\n",
    "\n",
    "########\n",
    "You can also customize the tokenization process to detect tokens on custom characters. This is often used for hyphenated words, which are words joined with hyphen. For example, “London-based” is a hyphenated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Internet\n",
      "protocol\n",
      "suite\n",
      "conceptual\n",
      "model\n",
      "set\n",
      "communications\n",
      "protocols\n",
      "Internet\n",
      "similar\n",
      "computer\n",
      "networks\n",
      ".\n",
      "\n",
      "              \n",
      "commonly\n",
      "known\n",
      "TCP\n",
      "/\n",
      "IP\n",
      "foundational\n",
      "protocols\n",
      "suite\n",
      "\n",
      "              \n",
      "Transmission\n",
      "Control\n",
      "Protocol\n",
      "(\n",
      "TCP\n",
      ")\n",
      "Internet\n",
      "Protocol\n",
      "(\n",
      "IP\n",
      ")\n",
      ".\n",
      "development\n",
      ",\n",
      "\n",
      "              \n",
      "versions\n",
      "known\n",
      "Department\n",
      "Defense\n",
      "(\n",
      "DoD\n",
      ")\n",
      "model\n",
      "development\n",
      "\n",
      "              \n",
      "networking\n",
      "method\n",
      "funded\n",
      "bythe\n",
      "United\n",
      "States\n",
      "Department\n",
      "Defense\n",
      "DARPA\n",
      ".\n",
      "implementation\n",
      "protocol\n",
      "stack\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "     if not token.is_stop:\n",
    "        print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gus Gus\n",
      "is be\n",
      "helping help\n",
      "organize organize\n",
      "a a\n",
      "developerconference developerconference\n",
      "on on\n",
      "Applications Applications\n",
      "of of\n",
      "Natural Natural\n",
      "Language Language\n",
      "Processing Processing\n",
      ". .\n",
      "He -PRON-\n",
      "keeps keep\n",
      "organizing organize\n",
      "local local\n",
      "Python Python\n",
      "meetups meetup\n",
      "and and\n",
      "several several\n",
      "internal internal\n",
      "talks talk\n",
      "at at\n",
      "his -PRON-\n",
      "workplace workplace\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "conference_help_text = ('Gus is helping organize a developer'\n",
    "     'conference on Applications of Natural Language'\n",
    "     ' Processing. He keeps organizing local Python meetups'\n",
    "     ' and several internal talks at his workplace.')\n",
    "conference_help_doc = nlp(conference_help_text)\n",
    "for token in conference_help_doc:\n",
    "     print (token, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "complete_text = ('Gus Proto is a Python developer currently'\n",
    "     'working for a London-based Fintech company. He is'\n",
    "     ' interested in learning Natural Language Processing.'\n",
    "     ' There is a developer conference happening on 21 July'\n",
    "     ' 2019 in London. It is titled \"Applications of Natural'\n",
    "     ' Language Processing\". There is a helpline number '\n",
    "     ' available at +1-1234567891. Gus is helping organize it.'\n",
    "     ' He keeps organizing local Python meetups and several'\n",
    "     ' internal talks at his workplace. Gus is also presenting'\n",
    "     ' a talk. The talk will introduce the reader about \"Use'\n",
    "     ' cases of Natural Language Processing in Fintech\".'\n",
    "     ' Apart from his work, he is very passionate about music.'\n",
    "     ' Gus is learning to play the Piano. He has enrolled '\n",
    "     ' himself in the weekend batch of Great Piano Academy.'\n",
    "     ' Great Piano Academy is situated in Mayfair or the City'\n",
    "     ' of London and has world-class piano instructors.')\n",
    "\n",
    "complete_doc = nlp(complete_text)\n",
    "# Remove stop words and punctuation symbols\n",
    "words = [token.text for token in complete_doc\n",
    "          if not token.is_stop and not token.is_punct]\n",
    "word_freq = Counter(words)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words = word_freq.most_common(5)\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Proto', 'currentlyworking', 'based', 'company', 'interested', 'conference', 'happening', '21', 'July', '2019', 'titled', 'Applications', 'helpline', 'number', 'available', '+1', '1234567891', 'helping', 'organize', 'keeps', 'organizing', 'local', 'meetups', 'internal', 'talks', 'workplace', 'presenting', 'introduce', 'reader', 'Use', 'cases', 'Apart', 'work', 'passionate', 'music', 'play', 'enrolled', 'weekend', 'batch', 'situated', 'Mayfair', 'City', 'world', 'class', 'piano', 'instructors']\n"
     ]
    }
   ],
   "source": [
    "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
    "print (unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]\n"
     ]
    }
   ],
   "source": [
    "words_all = [token.text for token in complete_doc if not token.is_punct]\n",
    "word_freq_all = Counter(words_all)\n",
    "# 5 commonly occurring words with their frequencies\n",
    "common_words_all = word_freq_all.most_common(5)\n",
    "print (common_words_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" `` PUNCT opening quotation mark\n",
      "The DT DET determiner\n",
      "Internet NN NOUN noun, singular or mass\n",
      "protocol NN NOUN noun, singular or mass\n",
      "suite NN NOUN noun, singular or mass\n",
      "is VBZ AUX verb, 3rd person singular present\n",
      "the DT DET determiner\n",
      "conceptual JJ ADJ adjective\n",
      "model NN NOUN noun, singular or mass\n",
      "and CC CCONJ conjunction, coordinating\n",
      "set NN NOUN noun, singular or mass\n",
      "of IN ADP conjunction, subordinating or preposition\n",
      "communications NNS NOUN noun, plural\n",
      "protocols NNS NOUN noun, plural\n",
      "used VBN VERB verb, past participle\n",
      "in IN ADP conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "Internet NN NOUN noun, singular or mass\n",
      "and CC CCONJ conjunction, coordinating\n",
      "similar JJ ADJ adjective\n",
      "computer NN NOUN noun, singular or mass\n",
      "networks NNS NOUN noun, plural\n",
      ". . PUNCT punctuation mark, sentence closer\n",
      "\n",
      "               _SP SPACE None\n",
      "It PRP PRON pronoun, personal\n",
      "is VBZ AUX verb, 3rd person singular present\n",
      "commonly RB ADV adverb\n",
      "known VBN VERB verb, past participle\n",
      "as IN SCONJ conjunction, subordinating or preposition\n",
      "TCP NNP PROPN noun, proper singular\n",
      "/ SYM SYM symbol\n",
      "IP NNP PROPN noun, proper singular\n",
      "because IN SCONJ conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "foundational JJ ADJ adjective\n",
      "protocols NNS NOUN noun, plural\n",
      "in IN ADP conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "suite NN NOUN noun, singular or mass\n",
      "are VBP AUX verb, non-3rd person singular present\n",
      "\n",
      "               _SP SPACE None\n",
      "the DT DET determiner\n",
      "Transmission NNP PROPN noun, proper singular\n",
      "Control NNP PROPN noun, proper singular\n",
      "Protocol NNP PROPN noun, proper singular\n",
      "( -LRB- PUNCT left round bracket\n",
      "TCP NNP PROPN noun, proper singular\n",
      ") -RRB- PUNCT right round bracket\n",
      "and CC CCONJ conjunction, coordinating\n",
      "the DT DET determiner\n",
      "Internet NNP PROPN noun, proper singular\n",
      "Protocol NNP PROPN noun, proper singular\n",
      "( -LRB- PUNCT left round bracket\n",
      "IP NNP PROPN noun, proper singular\n",
      ") -RRB- PUNCT right round bracket\n",
      ". . PUNCT punctuation mark, sentence closer\n",
      "During IN ADP conjunction, subordinating or preposition\n",
      "its PRP$ DET pronoun, possessive\n",
      "development NN NOUN noun, singular or mass\n",
      ", , PUNCT punctuation mark, comma\n",
      "\n",
      "               _SP SPACE None\n",
      "versions NNS NOUN noun, plural\n",
      "of IN ADP conjunction, subordinating or preposition\n",
      "it PRP PRON pronoun, personal\n",
      "were VBD AUX verb, past tense\n",
      "known VBN VERB verb, past participle\n",
      "as IN SCONJ conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "Department NNP PROPN noun, proper singular\n",
      "of IN ADP conjunction, subordinating or preposition\n",
      "Defense NNP PROPN noun, proper singular\n",
      "( -LRB- PUNCT left round bracket\n",
      "DoD NNP PROPN noun, proper singular\n",
      ") -RRB- PUNCT right round bracket\n",
      "model NN NOUN noun, singular or mass\n",
      "because IN SCONJ conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "development NN NOUN noun, singular or mass\n",
      "\n",
      "               _SP SPACE None\n",
      "of IN ADP conjunction, subordinating or preposition\n",
      "the DT DET determiner\n",
      "networking NN NOUN noun, singular or mass\n",
      "method NN NOUN noun, singular or mass\n",
      "was VBD AUX verb, past tense\n",
      "funded VBN VERB verb, past participle\n",
      "bythe IN ADP conjunction, subordinating or preposition\n",
      "United NNP PROPN noun, proper singular\n",
      "States NNP PROPN noun, proper singular\n",
      "Department NNP PROPN noun, proper singular\n",
      "of IN ADP conjunction, subordinating or preposition\n",
      "Defense NNP PROPN noun, proper singular\n",
      "through IN ADP conjunction, subordinating or preposition\n",
      "DARPA NNP PROPN noun, proper singular\n",
      ". . PUNCT punctuation mark, sentence closer\n",
      "Its PRP$ DET pronoun, possessive\n",
      "implementation NN NOUN noun, singular or mass\n",
      "is VBZ AUX verb, 3rd person singular present\n",
      "a DT DET determiner\n",
      "protocol JJ ADJ adjective\n",
      "stack NN NOUN noun, singular or mass\n",
      ". . PUNCT punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for token in about_doc:\n",
    "     print (token, token.tag_, token.pos_, spacy.explain(token.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_ lists the fine-grained part of speech.\n",
    "\n",
    "pos_ lists the coarse-grained part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "adjectives = []\n",
    "for token in about_doc:\n",
    "    if token.pos_ == 'NOUN':\n",
    "         nouns.append(token)\n",
    "    if token.pos_ == 'ADJ':\n",
    "         adjectives.append(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Internet,\n",
       " protocol,\n",
       " suite,\n",
       " model,\n",
       " set,\n",
       " communications,\n",
       " protocols,\n",
       " Internet,\n",
       " computer,\n",
       " networks,\n",
       " protocols,\n",
       " suite,\n",
       " development,\n",
       " versions,\n",
       " model,\n",
       " development,\n",
       " networking,\n",
       " method,\n",
       " implementation,\n",
       " stack]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[conceptual, similar, foundational, protocol]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization: Using displaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2bb0da93322c491ca48848661d1ffb26-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Natural</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Language</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Processing.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bb0da93322c491ca48848661d1ffb26-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bb0da93322c491ca48848661d1ffb26-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bb0da93322c491ca48848661d1ffb26-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bb0da93322c491ca48848661d1ffb26-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bb0da93322c491ca48848661d1ffb26-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bb0da93322c491ca48848661d1ffb26-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bb0da93322c491ca48848661d1ffb26-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bb0da93322c491ca48848661d1ffb26-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bb0da93322c491ca48848661d1ffb26-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bb0da93322c491ca48848661d1ffb26-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bb0da93322c491ca48848661d1ffb26-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bb0da93322c491ca48848661d1ffb26-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-2bb0da93322c491ca48848661d1ffb26-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-2bb0da93322c491ca48848661d1ffb26-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "about_interest_text = ('He is interested in learning'\n",
    "    ' Natural Language Processing.')\n",
    "about_interest_doc = nlp(about_interest_text)\n",
    "#displacy.serve(about_interest_doc, style='dep')\n",
    "displacy.render(about_interest_doc, style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Piano Academy 0 19 ORG Companies, agencies, institutions, etc.\n",
      "Mayfair 35 42 PERSON People, including fictional\n",
      "the City of London 46 64 GPE Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "piano_class_text = ('Great Piano Academy is situated'\n",
    "     ' in Mayfair or the City of London and has'\n",
    "     ' world-class piano instructors.')\n",
    "piano_class_doc = nlp(piano_class_text)\n",
    "for ent in piano_class_doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char,\n",
    "          ent.label_, spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, ent is a Span object with various attributes:\n",
    "\n",
    "text gives the Unicode text representation of the entity.\n",
    "start_char denotes the character offset for the start of the entity.\n",
    "end_char denotes the character offset for the end of the entity.\n",
    "label_ gives the label of the entity.\n",
    "spacy.explain gives descriptive details about an entity label. The spaCy model has a pre-trained list of entity classes. You can use displaCy to visualize these entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayaz/Desktop/Applied Informatics/sklearn/skenv/lib/python3.7/site-packages/spacy/displacy/__init__.py:94: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Great Piano Academy\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is situated in \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Mayfair\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " or \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the City of London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and has world-class piano instructors.</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "displacy.serve(piano_class_doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
